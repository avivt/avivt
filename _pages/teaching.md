---
title: "Aviv Tamar - Teaching"
layout: gridlay
excerpt: "Aviv Tamar -- Teaching."
sitemap: false
permalink: /teaching/
---


# Teaching

## 046194 Planning and Learning in Dynamical Systems 
## (a.k.a. Reinforcement Learning)

Resources: 
<a href="{{ site.url }}{{ site.baseurl }}/downloads/RL_lectures_July_19.pdf">Lecture notes</a> by S. Mannor, N. Shimkin, A. Tamar
<a href="https://arxiv.org/abs/1904.07272">Introduction to Multi Armed Bandits</a> by A. Slivkins

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-0pky">Week</th>
    <th class="tg-0pky">Lecture</th>
    <th class="tg-0lax">Tutorial</th>
    <th class="tg-0pky">Reference</th>
  </tr>
  <tr>
    <td class="tg-0pky">1</td>
    <td class="tg-0pky">Overview and examples<br>Dynamic Programming (DP): basic ideas</td>
    <td class="tg-0lax">DP - knapsack, LCS</td>
    <td class="tg-0pky">Ch. 1, 3.1 in Lecture Notes</td>
  </tr>
  <tr>
    <td class="tg-0pky">2</td>
    <td class="tg-0pky">Deterministic Controlled Processes <br>Finite horizon DP (for deterministic process)</td>
    <td class="tg-0lax">Fixed horizon DP, TSP</td>
    <td class="tg-0pky">Ch. 2 in Lecture Notes</td>
  </tr>
  <tr>
    <td class="tg-0pky">3</td>
    <td class="tg-0pky">Shortest-path problems <br>Graph search algorithms: Bellman-Ford, Dijkstra, A* <br>Deterministic continuous control: LQR, iLQR</td>
    <td class="tg-0lax">Shortest path on graph<br>Dijkstra proof</td>
    <td class="tg-0pky">Ch. 3.2, 3.3 in Lecture Notes</td>
  </tr>
  <tr>
    <td class="tg-0pky">4</td>
    <td class="tg-0pky">Markov Decision Processes (MDP)<br>Finite horizon DP (for MDPs)</td>
    <td class="tg-0lax">LQR</td>
    <td class="tg-0pky">Ch. 4 in Lecture Notes</td>
  </tr>
  <tr>
    <td class="tg-0pky">5</td>
    <td class="tg-0pky">Finite horizon DP (cont'd) <br>Discounted MDPs</td>
    <td class="tg-0lax">Markov chains<br>Viterbi algorithm</td>
    <td class="tg-0pky">Ch. 5 in Lecture Notes</td>
  </tr>
  <tr>
    <td class="tg-0pky">6</td>
    <td class="tg-0pky">Discounted MDPs (cont'd)<br>Value Iteration</td>
    <td class="tg-0lax">VI, Bellman operator</td>
    <td class="tg-0pky">Ch. 5 in Lecture Notes</td>
  </tr>
  <tr>
    <td class="tg-0pky">7</td>
    <td class="tg-0pky">Policy Iteration<br>Linear Programming formulations</td>
    <td class="tg-0lax">Robust DP<br>PI vs. VI</td>
    <td class="tg-0pky">Ch. 5 in Lecture Notes</td>
  </tr>
  <tr>
    <td class="tg-0pky">8</td>
    <td class="tg-0pky">Model free learning (small state spaces): TD(0), Q-learning, SARSA</td>
    <td class="tg-0lax">TD(0)<br>TD(lambda)</td>
    <td class="tg-0pky">Ch. 6 in Lecture Notes</td>
  </tr>
  <tr>
    <td class="tg-0pky">9</td>
    <td class="tg-0pky">MDPs with large state spaces - value-based approximations <br>projected Bellman equation<br>Policy evaluation algorithms: regression, TD(0), LSTD<br>Approximate policy iteration: LSPI, SARSA<br>Approximate value iteration: fitted-Q, Q-learning</td>
    <td class="tg-0lax">LSTD <br>LSPI</td>
    <td class="tg-0pky">Ch. 9.1, 9.3 in Lecture Notes</td>
  </tr>
  <tr>
    <td class="tg-0pky">10</td>
    <td class="tg-0pky">Stochastic approximation<br>Convergence of RL algorithms</td>
    <td class="tg-0lax">Stochastic approximation</td>
    <td class="tg-0pky">Ch. 7,8 in Lecture Notes</td>
  </tr>
  <tr>
    <td class="tg-0pky">11</td>
    <td class="tg-0pky">MDPs with large state spaces - policy-based approximations<br>Policy search <br>Policy gradients<br>Actor-critic</td>
    <td class="tg-0lax">Policy gradients</td>
    <td class="tg-0pky">Ch. 10 in Lecture Notes</td>
  </tr>
  <tr>
    <td class="tg-0pky">12</td>
    <td class="tg-0pky">Multi-armed bandits in the stationary arm model <br>Regret, Hoeffding inequality, uniform exploration, adaptive exploration</td>
    <td class="tg-0lax">UCB</td>
    <td class="tg-0pky">Ch. 1 in Slivkins' book</td>
  </tr>
  <tr>
    <td class="tg-0pky">13</td>
    <td class="tg-0pky">MDPs with large state spaces - Monte Carlo Tree Search (UCT)<br>Deep RL (overview)</td>
    <td class="tg-0lax">TD(0)+FA convergence</td>
    <td class="tg-0pky">Ch. 9.2 in Lecture Notes</td>
  </tr>
</table>